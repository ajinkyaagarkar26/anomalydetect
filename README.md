# Anomaly Detection Project

## Quick Start
```bash
python3 data_process.py
python3 deeplog.py vocab
python3 deeplog.py train
python3 deeplog.py predict
```

## What does the Drain.py parse method do?

The `parse` method in `Drain.py` is the **main entry point** of the Drain log parsing algorithm. It implements an **online log clustering algorithm** that groups similar log messages together and extracts templates for anomaly detection.

### Key Steps:

1. **Initialization**
   - Creates a root node for the prefix tree structure
   - Initializes log cluster list to store grouped logs

2. **Data Loading & Preprocessing**
   - Loads the log file using specified log format
   - Converts raw log lines into structured DataFrame
   - Applies regex patterns to extract fields (timestamp, component, content, etc.)

3. **Main Processing Loop**
   For each log message:
   
   **a) Preprocessing:**
   - Extracts log content and applies regex substitutions
   - Splits the message into tokens (words)
   
   **b) Tree Search:**
   - Searches the prefix tree to find existing similar log templates
   - Uses tree structure for efficient pattern matching
   
   **c) Clustering Decision:**
   - **If no match found:** Creates new log cluster with this message as template
   - **If match found:** Merges with existing cluster, updates template by replacing variable parts with wildcards `<*>`

4. **Output Generation**
   - Creates `*_structured.csv`: Original logs with assigned EventIds and templates
   - Creates `*_templates.csv`: Unique templates with occurrence counts

### Algorithm Features:

- **Prefix Tree Structure**: Groups logs by length, then by tokens
- **Similarity Matching**: Uses similarity threshold to determine cluster membership
- **Wildcard Generation**: Automatically replaces variable parts (IDs, timestamps) with `<*>`
- **Online Processing**: Processes logs incrementally, building templates in real-time

### Example:
```
Input logs:
"User john logged in from 192.168.1.1"
"User mary logged in from 192.168.1.5" 
"User bob logged in from 10.0.0.1"

Output template:
"User <*> logged in from <*>"
```

This structured template extraction is essential for the downstream anomaly detection models to identify patterns and detect anomalies in log sequences.

## What does the mapping() method in data_process.py do?

The `mapping()` function is a **template-to-numeric conversion** step that prepares log templates for machine learning models by converting them into numerical identifiers.

### Key Steps:

1. **Load Template Data**
   ```python
   log_temp = pd.read_csv(log_templates_file)
   ```
   - Reads the `*_templates.csv` file generated by the Drain parser
   - Contains EventId, EventTemplate, and Occurrences columns

2. **Sort by Frequency**
   ```python
   log_temp.sort_values(by = ["Occurrences"], ascending=False, inplace=True)
   ```
   - Sorts templates by occurrence count (most frequent first)
   - This prioritizes common log patterns for better model training

3. **Create Mapping Dictionary**
   ```python
   log_temp_dict = {event: idx+1 for idx , event in enumerate(list(log_temp["EventId"])) }
   ```
   - Maps each EventId (hash) to a sequential integer (1, 2, 3, ...)
   - Most frequent templates get lower numbers (better for neural networks)

4. **Save Mapping**
   ```python
   with open (output_dir + "deeplog_log_templates.json", "w") as f:
       json.dump(log_temp_dict, f)
   ```
   - Saves the mapping as JSON file for later use during training/prediction

### Purpose:
- **Neural Network Compatibility**: Converts string EventIds to integers for embedding layers
- **Frequency-Based Ordering**: Assigns lower numbers to frequent patterns (improves model convergence)
- **Consistent Mapping**: Ensures same EventId always maps to same number across train/test

### Example:
```
Input templates (sorted by frequency):
EventId: "a1b2c3d4", Template: "User <*> logged in", Occurrences: 1000
EventId: "e5f6g7h8", Template: "Error in <*>", Occurrences: 500
EventId: "i9j0k1l2", Template: "System started", Occurrences: 50

Output mapping:
{
  "a1b2c3d4": 1,  # Most frequent gets ID 1
  "e5f6g7h8": 2,  # Second most frequent gets ID 2
  "i9j0k1l2": 3   # Least frequent gets ID 3
}
```

This numeric mapping is essential for the DeepLog model to process log sequences as integer sequences for anomaly detection.

## Components


